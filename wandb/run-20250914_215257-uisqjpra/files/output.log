[*] Setting Randomness...
[2025-09-14 21:52:58,122][jax._src.xla_bridge][INFO] - Unable to initialize backend 'cuda':
[2025-09-14 21:52:58,123][jax._src.xla_bridge][INFO] - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
[2025-09-14 21:52:58,129][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-09-14 21:52:58,130][jax._src.xla_bridge][WARNING] - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
[*] Generating MNIST Classification Dataset...
[*] Starting `s4` Training on `mnist-classification` =>> Initializing...
/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py:131: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree.leaves (jax v0.4.25 or newer) or jax.tree_util.tree_leaves (any JAX version).
  extra_keys = set(lr_layer.keys()) - set(jax.tree_leaves(name_map(params)))
/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py:143: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree.leaves (jax v0.4.25 or newer) or jax.tree_util.tree_leaves (any JAX version).
  print(f"[*] Trainable Parameters: {sum(jax.tree_leaves(param_sizes))}")
[*] Trainable Parameters: 397834
[*] Total training steps: 4690
[*] Starting Training Epoch 1...
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [36:16<00:00,  4.64s/it]
[*] Running Epoch 1 Validation...
  0%|                                                                                                         | 0/79 [00:00<?, ?it/s]
Error executing job with overrides: []
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 465, in main
    example_train(**cfg)
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 366, in example_train
    test_loss, test_acc = validate(
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 190, in validate
    loss, acc = eval_step(
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 252, in eval_step
    logits = model.apply({"params": params}, batch_inputs)
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/s4.py", line 617, in __call__
    x = layer(x)
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/s4.py", line 558, in __call__
    x = random_bitflip(x, p=1e-3)
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/s4.py", line 109, in random_bitflip
    if np.random.rand() > p:
  File "/home/jaxon/miniconda3/envs/ssm-faults/lib/python3.9/site-packages/jax/_src/deprecations.py", line 55, in getattr
    raise AttributeError(f"module {module!r} has no attribute {name!r}")
AttributeError: module 'jax.numpy' has no attribute 'random'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
