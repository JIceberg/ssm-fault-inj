[*] Setting Randomness...
[2025-09-14 18:08:27,529][jax._src.xla_bridge][INFO] - Unable to initialize backend 'cuda':
[2025-09-14 18:08:27,530][jax._src.xla_bridge][INFO] - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
[2025-09-14 18:08:27,532][jax._src.xla_bridge][INFO] - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
[2025-09-14 18:08:27,533][jax._src.xla_bridge][WARNING] - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.
[*] Generating MNIST Classification Dataset...
[*] Starting `s4` Training on `mnist-classification` =>> Initializing...
/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py:131: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree.leaves (jax v0.4.25 or newer) or jax.tree_util.tree_leaves (any JAX version).
  extra_keys = set(lr_layer.keys()) - set(jax.tree_leaves(name_map(params)))
/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py:143: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree.leaves (jax v0.4.25 or newer) or jax.tree_util.tree_leaves (any JAX version).
  print(f"[*] Trainable Parameters: {sum(jax.tree_leaves(param_sizes))}")
[*] Trainable Parameters: 397834
[*] Total training steps: 4690
[*] Starting Training Epoch 1...
100%|██████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [35:58<00:00,  4.60s/it]
[*] Running Epoch 1 Validation...
  0%|                                                                                                         | 0/79 [00:00<?, ?it/s]
Error executing job with overrides: []
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 465, in main
    example_train(**cfg)
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 366, in example_train
    test_loss, test_acc = validate(
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 190, in validate
    loss, acc = eval_step(
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/train.py", line 252, in eval_step
    logits = model.apply({"params": params}, batch_inputs, rngs={"fault": rng})
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/s4.py", line 622, in __call__
    x = layer(x)
  File "/home/jaxon/research/fault-injection/ssm-fault-inj/s4/s4.py", line 562, in __call__
    rng = self.make_rng("fault")
flax.errors.InvalidRngError: layers_0 needs PRNG for "fault" (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.InvalidRngError)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
